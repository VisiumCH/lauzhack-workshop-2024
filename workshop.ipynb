{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG System Locally with Ollama, LlamaIndex, and Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0 - Install Workshop Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the workshop, ensure all necessary dependencies are installed in your Python environment. Use the following steps to set up your environment.\n",
    "\n",
    "### Step 1: Create a Virtual Environment\n",
    "\n",
    "Create and activate a virtual environment to isolate the workshop dependencies. For this workshop, we use **Python 3.11**. Choose between **venv** or **conda** (using Mamba for efficiency).\n",
    "\n",
    "##### Using `venv`\n",
    "\n",
    "On Linux/Mac:\n",
    "  ```bash\n",
    "  python3.11 -m venv local-rag\n",
    "  source local-rag/bin/activate\n",
    "  ```\n",
    "On Windows:\n",
    "  ```bash\n",
    "  python3.11 -m venv local-rag\n",
    "  local-rag\\Scripts\\activate\n",
    "  ```\n",
    "\n",
    "##### Using `conda`\n",
    "\n",
    "   ```bash\n",
    "   conda create -n local-rag python=3.11\n",
    "   conda activate local-rag\n",
    "   ```\n",
    "\n",
    "### Step 2: Install Required Packages\n",
    "\n",
    "Install all the required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 3: Verify Installation\n",
    "\n",
    "Check that the key packages are installed correctly by importing them in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import llama_index\n",
    "import ollama\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Setting up Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Ollama\n",
    "\n",
    "First, download and install Ollama from the official website: [https://ollama.com/download/](https://ollama.com/download/).\n",
    "\n",
    "### Pull Required Models\n",
    "\n",
    "Open a terminal and run the following commands to download the necessary models:\n",
    "\n",
    "1. Pull the `llama3` model:\n",
    "   ```bash\n",
    "   ollama pull llama3\n",
    "   ```\n",
    "\n",
    "2. Pull the Nomic embedding model if required:\n",
    "   ```bash\n",
    "   ollama pull nomic\n",
    "   ```\n",
    "\n",
    "### Run the Model\n",
    "\n",
    "Once the models are installed, you can run the `llama3` model and test it by writing some prompts. Use the following command:\n",
    "\n",
    "```bash\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "Type a prompt and observe the output to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with Ollama in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(model=\"llama3\", prompt=\"What is EPFL?\", stream=True)\n",
    "\n",
    "for r in response:\n",
    "    print(r[\"response\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Getting Started with LlamaIndex and ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LlamaIndex** ([official site](https://llamaindex.ai)) is a framework for connecting LLMs with data sources, enabling efficient retrieval and interaction with structured or unstructured data.\n",
    "\n",
    "**Chroma** ([official site](https://www.trychroma.com)) is a vector database designed for managing embeddings and serving as a retrieval layer for LLM applications.\n",
    "\n",
    "In this exercise, weâ€™ll explore how to set up and use LlamaIndex to index and retrieve data in a **Chroma** database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Let's download a PDF\n",
    "\n",
    "You can start by adding documents to the `./docs` folder. If you don't know what to use, we suggest downloading the PDF at the following link:\n",
    "\n",
    "https://observationofalostsoul.wordpress.com/wp-content/uploads/2011/05/the-gospel-of-the-flying-spaghetti-monster.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Chroma as the Storage Backend\n",
    "\n",
    "Initialize the Chroma database and configure it for use with LlamaIndex. Here, we create an **Ephemeral Client** and collection, which stores data temporarily in memory without persisting it. This is ideal for testing and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"mydocs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a **Persistent Client** that will preserve your database across sessions with:\n",
    "\n",
    "```python\n",
    "client = chromadb.PersistentClient(path=\"/path/to/save/to\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Up LlamaIndex connectors\n",
    "\n",
    "Configure LlamaIndex to connect with Chroma as the vector store and set up a storage context. A **storage context** is an abstraction that manages how data is stored and retrieved, enabling seamless integration with different storage backends like Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load and explore documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use LlamaIndex's `SimpleDirectoryReader` to **ingest documents from a directory**. This utility reads files from a specified directory and prepares them for indexing by splitting the content into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./docs\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the content of the documents further with a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "def data_to_df(nodes: List[TextNode]):\n",
    "    \"\"\"Convert a list of TextNode objects to a pandas DataFrame.\"\"\"\n",
    "    return pd.DataFrame([node.dict() for node in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = data_to_df(documents)\n",
    "\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe several attributes, including `metadata`, `text`, `text_template`, and others. Let's focus on these three key categories:\n",
    "\n",
    "- **`metadata`**: This attribute contains additional information about the document, such as its source, creation date, or tags that can be used for filtering or retrieval purposes.\n",
    "- **`text`**: The main content of the document, representing the raw textual data that will be indexed and queried.\n",
    "- **`text_template`**: A structured format or schema for the document's text, often used to define how the content should be presented or processed during queries. \n",
    "\n",
    "These attributes play distinct roles in organizing and interacting with your data. Feel free to explore the different attributes at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Index and the documents\n",
    "\n",
    "To ingest documents into an index, we will need an embedder model to convert the document content into vector representations. These embeddings enable efficient similarity searches and retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LlamaIndex, we can create an index using the `VectorStoreIndex` class, which enables efficient storage and retrieval of document embeddings and integrates with various storage backends and embedding models. We use here the chroma collection we previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Query the Index for Retrieval\n",
    "\n",
    "Once the documents are indexed, we can perform retrieval on them. This allows us to ask questions or search for relevant content based on the embeddings stored in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "nodes_with_score = retriever.retrieve(\"What is the Flying Spaghetti Monster?\")\n",
    "nodes = [n.node for n in nodes_with_score]\n",
    "data_to_df(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've retrieved your first data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Your First RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Retrieval-Augmented Generation (RAG) system, you need a Large Language Model (LLM) to generate answers to your queries by combining retrieved knowledge with the model's reasoning capabilities. At this point, Ollama comes to help as the LLM powering your RAG system. We set it up for LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready for querying your data. You can define a query engine and start asking it questions. Congrats, You have a working RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is the Flying Spaghetti Monster?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "LlamaIndex offers an easy way to improve the generated answer by prompting the LLM with a custom template, in which the relevant context will be fed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"As a devoted Pastafarian scholar touched by His Noodly Appendage, you shall defend our pasta-based teachings.\\n\\n\"\n",
    "    \"Sacred commandments:\\n\"\n",
    "    \"- Keep answers CONCISE (2-3 paragraphs max)\\n\"\n",
    "    \"- Cite the Sacred Texts below\\n\"\n",
    "    \"- Use pasta metaphors liberally\\n\"\n",
    "    \"- Defend with noodly passion\\n\\n\"\n",
    "    \"Sacred texts:\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------------------\\n\\n\"\n",
    "    \"Question from seeker:\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer with fervor, citing texts. Be passionate but brief. R'amen.\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similartiy_top_k=3,\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is the Flying Spaghetti Monster?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, a basic retriever is used. Let's look at how data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_score = response.source_nodes\n",
    "nodes = [n.node for n in nodes_with_score]\n",
    "data_to_df(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "node = nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do the models see exactly? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    node.get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    node.get_content(metadata_mode=MetadataMode.LLM),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to change the embeddings. For example, we can split the sentences in smaller blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index data\n",
    "index.vector_store.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter, SentenceWindowNodeParser\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=200)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True,\n",
    "    transformations=[sentence_splitter],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many more ways to improve the RAG system, explore them on the official LlamaIndex page!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauzhack-workshop-2024-7gra-v0p-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

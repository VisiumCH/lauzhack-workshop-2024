{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Get started with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Download and install ollama from here\n",
    "https://ollama.com/download/\n",
    "\n",
    "As part of installing the python environment, you already installed the [ollama](https://pypi.org/project/ollama/) python package.\n",
    "\n",
    "Run the next commands in a Terminal.\n",
    "\n",
    "Now download and install the llama3 and the nomic embed model: \n",
    "```\n",
    "ollama pull llama3\n",
    "```\n",
    "Run the llama3 model and write some prompts:\n",
    "```\n",
    "ollama run llama3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with Ollama in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also interact with Ollama from Python, e.g. using the LlamaIndex Framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama\n",
    "\n",
    "# Language model from Ollama\n",
    "# llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "llm = Ollama(model=\"llama3\", request_timeout=120.0)\n",
    "\n",
    "# Set it as the default LLM in LlamaIndex\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is EPFL?\"\n",
    "response = llm.stream_complete(prompt)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Create a Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load an Word Embedding model from Huggingface. We will be using BAAI for this workshop, but any other model would be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings model from HuggingFace\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Set it as the default embedding model in LlamaIndex\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading documents with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use LlamaIndex to read our documents and parse them. Make sure the the Gospel of the Flying spaghetti monster pdf is in the `/docs` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./docs\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now embed our documents and store them in a ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ephermeral client for Chroma\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "# chroma_collection = chroma_client.create_collection(\"mydocs\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"mydocs\")\n",
    "\n",
    "# Vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# Storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# use this to set custom chunk size and splitting\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chroma_collection.get()\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"IDs\": result[\"ids\"],\n",
    "    \"Documents\": result[\"documents\"],\n",
    "    \"Metadata\": result[\"metadatas\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"metadatas\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Your First RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first practice the retrieval of a document based on a the similarity with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "retriever.retrieve(\"What is Red Teaming?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query our LLM directly, using the retrieved documents as source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similartiy_top_k=3,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is Red Teaming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which sources were identified to be the most relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get_formatted_sources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similartiy_top_k=3,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is Red Teaming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similartiy_top_k=3,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is Red Teaming?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's practice the reprompting of our LLM with a custom template, in which the relevant context will be fed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# custome prompt template\n",
    "template = (\n",
    "    \"Imagine you are an advanced AI expert in cyber security laws, with access to all current and relevant legal documents, \"\n",
    "    \"case studies, and expert analyses. Your goal is to provide insightful, accurate, and concise answers to questions in this domain.\\n\\n\"\n",
    "    \"Here is some context related to the query:\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"Considering the above information, please respond to the following inquiry with detailed references to applicable laws, \"\n",
    "    \"precedents, or principles where appropriate:\\n\\n\"\n",
    "    \"Question: {query_str}\\n\\n\"\n",
    "    \"Answer succinctly, starting with the phrase 'According to cyber security law,' and ensure your response is understandable to someone without a legal background.\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similartiy_top_k=3,\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is Red Teaming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauzhack-workshop-2024-7gra-v0p-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
